{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Pan/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Activation, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>id_str</th>\n",
       "      <th>in_reply_to_user_id_str</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01 13:37:52</td>\n",
       "      <td>51473</td>\n",
       "      <td>947824196909961216</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>8237</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Will be leaving Florida for Washington (D.C.) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01 12:44:40</td>\n",
       "      <td>53557</td>\n",
       "      <td>947810806430826496</td>\n",
       "      <td>25073877.0</td>\n",
       "      <td>False</td>\n",
       "      <td>14595</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Iran is failing at every level despite the ter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01 12:12:00</td>\n",
       "      <td>138808</td>\n",
       "      <td>947802588174577664</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>49566</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>The United States has foolishly given Pakistan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-31 23:43:04</td>\n",
       "      <td>154769</td>\n",
       "      <td>947614110082043904</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>35164</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>HAPPY NEW YEAR! We are MAKING AMERICA GREAT AG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-12-31 22:18:20</td>\n",
       "      <td>157655</td>\n",
       "      <td>947592785519173632</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>39428</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>As our Country rapidly grows stronger and smar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           created_at  favorite_count              id_str  \\\n",
       "0 2018-01-01 13:37:52           51473  947824196909961216   \n",
       "1 2018-01-01 12:44:40           53557  947810806430826496   \n",
       "2 2018-01-01 12:12:00          138808  947802588174577664   \n",
       "3 2017-12-31 23:43:04          154769  947614110082043904   \n",
       "4 2017-12-31 22:18:20          157655  947592785519173632   \n",
       "\n",
       "   in_reply_to_user_id_str is_retweet  retweet_count              source  \\\n",
       "0                      NaN      False           8237  Twitter for iPhone   \n",
       "1               25073877.0      False          14595  Twitter for iPhone   \n",
       "2                      NaN      False          49566  Twitter for iPhone   \n",
       "3                      NaN      False          35164  Twitter for iPhone   \n",
       "4                      NaN      False          39428  Twitter for iPhone   \n",
       "\n",
       "                                                text  \n",
       "0  Will be leaving Florida for Washington (D.C.) ...  \n",
       "1  Iran is failing at every level despite the ter...  \n",
       "2  The United States has foolishly given Pakistan...  \n",
       "3  HAPPY NEW YEAR! We are MAKING AMERICA GREAT AG...  \n",
       "4  As our Country rapidly grows stronger and smar...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the 2017 data to see the original data structure\n",
    "temp=pd.read_json(\"./jsondata/condensed_2017.json\")\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From Donald Trump: Wishing everyone a wonderfu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump International Tower in Chicago ranked 6t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wishing you and yours a very Happy and Bountif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Donald Trump Partners with TV1 on New Reality ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>--Work has begun, ahead of schedule, to build ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  From Donald Trump: Wishing everyone a wonderfu...\n",
       "1  Trump International Tower in Chicago ranked 6t...\n",
       "2  Wishing you and yours a very Happy and Bountif...\n",
       "3  Donald Trump Partners with TV1 on New Reality ...\n",
       "4  --Work has begun, ahead of schedule, to build ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name=\"./jsondata/condensed_\"\n",
    "mydf = pd.read_json(file_name+str(2000+9)+\".json\")\n",
    "mydf = mydf.loc[mydf['is_retweet']==False][['text']]\n",
    "\n",
    "# read all year    \n",
    "for year in range(10,19):\n",
    "    df = pd.read_json(file_name+str(2000+year)+\".json\")\n",
    "    df = df.loc[df['is_retweet']==False][['text']]\n",
    "    mydf=mydf.append(df,ignore_index=True)\n",
    "mydf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From Donald Trump: Wishing everyone a wonderfu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump International Tower in Chicago ranked 6t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wishing you and yours a very Happy and Bountif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Donald Trump Partners with TV1 on New Reality ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Work has begun, ahead of schedule, to build th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  From Donald Trump: Wishing everyone a wonderfu...\n",
       "1  Trump International Tower in Chicago ranked 6t...\n",
       "2  Wishing you and yours a very Happy and Bountif...\n",
       "3  Donald Trump Partners with TV1 on New Reality ...\n",
       "4  Work has begun, ahead of schedule, to build th..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(mydf)):\n",
    "    # remove url\n",
    "    mydf['text'][i]= re.sub(r\"http\\S+\", \"\", mydf['text'][i])\n",
    "    \n",
    "    # remove these charcters (\" . -) at the beginning of the string \n",
    "    # remove (\") at the end of the string \n",
    "    # remove whitespaces at the beginning and the end\n",
    "    mydf['text'][i]= mydf['text'][i].strip().lstrip(\"-\").lstrip('\"').lstrip(\".\").rstrip('\"').strip()\n",
    "    \n",
    "mydf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32858"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mydf) #number of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test\n",
    "train=mydf.sample(frac=0.9,random_state=200)\n",
    "test=mydf.drop(train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test\n",
    "text_train=''\n",
    "text_val=''\n",
    "\n",
    "for i in train['text']:\n",
    "    text_train+=i+' '\n",
    "\n",
    "for i in test['text']:\n",
    "    text_val+=i+' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n,\\r, ,!,\",#,$,%,&,\\',(,),*,+,,,-,.,/,0,1,2,3,4,5,6,7,8,9,:,;,=,?,@,A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z,[,\\\\,],_,`,a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z,{,|,},~'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove non-English letters & emoji\n",
    "text_train=text_train.encode('ascii',errors='ignore').decode()\n",
    "text_val=text_val.encode('ascii',errors='ignore').decode()\n",
    "\n",
    "chars = sorted(list(set(text_train+text_val)))\n",
    "','.join(chars) #show remaining letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ,!,\",#,$,%,&,\\',,,.,0,1,2,3,4,5,6,7,8,9,:,;,?,@,A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z,_,a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove some more special characters to reduce the size of parameter matrix --> increase training spped\n",
    "text_train=re.sub(\"[-+~`*\\]\\[|\\\\n\\\\r\\(\\)\\{\\}<>]\", \"\", text_train)\n",
    "text_train=text_train.replace(\"/\",\"\").replace(\"\\\\\", \"\") .replace(\"=\", \"\").replace('\\'',\"'\")\n",
    "text_val=re.sub(\"[-+~`*\\]\\[|\\\\n\\\\r\\(\\)\\{\\}<>]\", \"\", text_val)\n",
    "text_val=text_val.replace(\"/\",\"\").replace(\"\\\\\", \"\") .replace(\"=\", \"\").replace('\\'',\"'\")\n",
    "\n",
    "chars = sorted(list(set(text_train+text_val)))\n",
    "','.join(chars)#show remaining letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 3458381 characters\n"
     ]
    }
   ],
   "source": [
    "text_train_len = len(text_train)\n",
    "text_val_len = len(text_val)\n",
    "print(\"Total of %d characters\" % (text_train_len + text_val_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary len = 77\n",
      "[' ', '!', '\"', '#', '$', '%', '&', \"'\", ',', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# generic vocabulary\n",
    "VOCABULARY_SIZE = len(chars)\n",
    "characters_to_ix = {c:i for i,c in enumerate(chars)}\n",
    "print(\"vocabulary len = %d\" % VOCABULARY_SIZE)\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@A_Beil   Thanks Andrew! Thank you Greensboro, North Carolina! Will be back soon! #AmericaFirst @Mr_president706: @realDonaldTrump please run for president, and show these so called Republicans how to get the job done. RT @CPACnews: ACU Announces @realDonaldTrump will be a featured speaker at #CPAC2013!  Get tickets today at @SAVEHICNO1: MR. TRUMP, I THINK YOU OUGHT TO DO AN APPRENTICE SHOW FOR PEOPLE TO COME UP FOR A WINNING STRADEGY FOR YOU TO RUN IN 2016 @HomesWestfield @EricTrump @MyFoxNY @rosannascotto @StJude  Thank you. @MotivationIdeas: Sometimes by losing a battle you find a new way to win the war.  Donald Trump The leader and negotiators representing Mexico are far smarter and more cunning than the leader and negotiators representing the U.S.! @rdowns I never went bankrupt. Enemies love to say I did. Didn't happen. @manc999 Write to Mark Burnett. I love Mexico but not the unfair trade deals that the US so stupidly makes with them. Really bad for US jobs, only good for Mexico.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_train[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_batch(X, y, samples=3):\n",
    "    for i in range(samples):\n",
    "        sentence = \"\"\n",
    "        for s in range(SEQUENCE_LEN):\n",
    "            sentence += chars[X[i,s,:].argmax()]\n",
    "        next_char = chars[y[i,:].argmax()]\n",
    "        \n",
    "        print(\"sample #%d: ...%s -> '%s'\" % (\n",
    "            i,\n",
    "            sentence[-20:],\n",
    "            next_char\n",
    "        ))\n",
    "\n",
    "# Generate batches for training\n",
    "def batch_generator(text, count):\n",
    "    while True: # keras wants that for reasons\n",
    "        for batch_ix in range(count):\n",
    "            X = np.zeros((BATCH_SIZE, SEQUENCE_LEN, VOCABULARY_SIZE))\n",
    "            y = np.zeros((BATCH_SIZE, VOCABULARY_SIZE))\n",
    "\n",
    "            batch_offset = BATCH_SIZE * batch_ix\n",
    "\n",
    "            for sample_ix in range(BATCH_SIZE):\n",
    "                sample_start = batch_offset + sample_ix\n",
    "                for s in range(SEQUENCE_LEN):\n",
    "                    X[sample_ix, s, characters_to_ix[text[sample_start+s]]] = 1\n",
    "                y[sample_ix, characters_to_ix[text[sample_start+s+1]]]=1\n",
    "\n",
    "            yield X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample #0: ...sboro, North Carolin -> 'a'\n",
      "sample #1: ...boro, North Carolina -> '!'\n",
      "sample #2: ...oro, North Carolina! -> ' '\n",
      "sample #3: ...ro, North Carolina!  -> 'W'\n",
      "sample #4: ...o, North Carolina! W -> 'i'\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 512\n",
    "SEQUENCE_LEN = 60\n",
    "\n",
    "# describe some samples from the first batch\n",
    "for ix, (X,y) in enumerate(batch_generator(text_train, count=1)):    \n",
    "    describe_batch(X, y, samples=5)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building\n",
    "reference: https://keras.io/getting-started/sequential-model-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    for i in range(LAYER_COUNT):\n",
    "        if (i!=(LAYER_COUNT-1)):\n",
    "            return_seq_flg=True\n",
    "        else:\n",
    "            return_seq_flg=False\n",
    "            \n",
    "        model.add(\n",
    "            LSTM(\n",
    "                HIDDEN_LAYERS_DIM, \n",
    "                return_sequences=return_seq_flg,\n",
    "                input_shape=(SEQUENCE_LEN, VOCABULARY_SIZE),)\n",
    "            )\n",
    "        \n",
    "        model.add(Dropout(DROPOUT))\n",
    "    \n",
    "    model.add(Dense(VOCABULARY_SIZE))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=\"adam\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "reference for EarlyStopping: https://stackoverflow.com/questions/43906048/keras-early-stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "HIDDEN_LAYERS_DIM = 512\n",
    "LAYER_COUNT = 4\n",
    "DROPOUT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training batch count: 6080\n",
      "validation batch count: 673\n"
     ]
    }
   ],
   "source": [
    "training_model = build_model()\n",
    "train_batch_count = (text_train_len - SEQUENCE_LEN) // BATCH_SIZE\n",
    "val_batch_count = (text_val_len - SEQUENCE_LEN) // BATCH_SIZE\n",
    "print(\"training batch count: %d\" % train_batch_count)\n",
    "print(\"validation batch count: %d\" % val_batch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "filepath = \"./1-gpu_BS-%d_%d-%s_dp%.2f_%dS_epoch{epoch:02d}-loss{loss:.4f}-val-loss{val_loss:.4f}_weights\" % (\n",
    "    BATCH_SIZE,\n",
    "    LAYER_COUNT,\n",
    "    HIDDEN_LAYERS_DIM,\n",
    "    DROPOUT,\n",
    "    SEQUENCE_LEN)\n",
    "\n",
    "# checkpoint\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath,\n",
    "    save_weights_only=True)\n",
    "\n",
    "#Â early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "# patience argument represents the number of epochs before stopping once your loss starts to \n",
    "# increase (stops improving)\n",
    "# comment: we thought we can run quite a lot epoch. However, the max epoch we ran was 10. Thus the \n",
    "# early_stopping function might not be utilized.\n",
    "\n",
    "callbacks_list = [checkpoint, early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "   1/6080 [..............................] - ETA: 96:03:46 - loss: 4.3439"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-7edb8f8232f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_batch_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_batch_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     initial_epoch=0)\n\u001b[0m",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1274\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2222\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2223\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2224\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2226\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = training_model.fit_generator(\n",
    "    batch_generator(text_train, count=train_batch_count),\n",
    "    train_batch_count,\n",
    "    max_queue_size=1, # no more than one queued batch in RAM\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks_list,\n",
    "    validation_data=batch_generator(text_val, count=val_batch_count),\n",
    "    validation_steps=val_batch_count,\n",
    "    initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydotplus\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/bf/62567830b700d9f6930e9ab6831d6ba256f7b0b730acb37278b0ccdffacf/pydotplus-2.0.2.tar.gz (278kB)\n",
      "\u001b[K    100% |################################| 286kB 5.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.1 in /Users/Pan/anaconda/lib/python3.6/site-packages (from pydotplus) (2.2.0)\n",
      "Building wheels for collected packages: pydotplus\n",
      "  Running setup.py bdist_wheel for pydotplus ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/Pan/Library/Caches/pip/wheels/35/7b/ab/66fb7b2ac1f6df87475b09dc48e707b6e0de80a6d8444e3628\n",
      "Successfully built pydotplus\n",
      "Installing collected packages: pydotplus\n",
      "Successfully installed pydotplus-2.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --user pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(training_model, to_file='./model.png', show_shapes=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
